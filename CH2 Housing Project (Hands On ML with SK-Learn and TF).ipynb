{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On ML with Scikit Learn and TF CH 2: End-to-End ML Project\n",
    "\n",
    "Currently learning all the necesary steps to take when taking on an ML Project.\n",
    "\n",
    "A Quick summary is shown below:\n",
    "\n",
    "1. First thing first, frame the problem and look at the bigger picture,\n",
    "2. Get Data\n",
    "3. Explore the data\n",
    "4. Prepare the data\n",
    "5. Short-List Promising models.\n",
    "6. Fine tune the system.\n",
    "7. Present the solution.\n",
    "8. Launch the solution.\n",
    "\n",
    "The book does go in more detail for each step, but I chose to summarise the main 8 steps, and I will be using them as a template for my projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Frame the problem and look at the bigger picture\n",
    "\n",
    "The problem I need to solve here is to be able to determine the median price of a house in a certain district. This price will be used to determine whether investors should invest in housing in certain districts. Currently, prices are maunally calculated using experts, this is time consuming and expensive for the company, so I need to develop a model that predict the price of a house based on its district.\n",
    "\n",
    "This a supervised learning problem, as I will be using the Califorinia Census Data to train the model. Further this is a regression type problem as the model has to predict a certain value. The performance measures to be used can be RMSE (Root Mean Square Error) or the MAE(Mean Absolute Error). We will have to look at the data to determine which one. \n",
    "\n",
    "There is human expertise available, as the preictions can be checked with experts to determine the accuracy of the model predictions within a certain accuracy margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Data\n",
    "I will download the data and load all the necessary libraries needed to perform the task at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "# VERY NB: It is preferable to always create a function to get data online, that we have the most updated version of the data\n",
    "\n",
    "housing_data = pd.read_csv(\"housing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Data (Exploratory DataAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to download the data rather, will learn to fetch data online, when doing another project. However, I have loaded the data and this is what I see:\n",
    "\n",
    "The data has 10 columns and 20640 entries(rows). All the columns are numbers expect for the ocean_proximity column. \n",
    "\n",
    "The target values for this project will be the median_house_value column. and the rest are features features I will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the numerical columns on a histogram to see how they are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.hist(bins = 20, figsize = (20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets the Median Income into categories.\n",
    "# We can see the the income for the districts are capped between 0 and 15. So in real life we might have to further research as to\n",
    "# How this value was calculated, but for now, will just divide the incomes into classes\n",
    "\n",
    "housing_data[\"income_cat\"] = np.ceil(housing_data['median_income']/1.5) # Divide income by 1.5 and then round up to have a fixed number\n",
    "housing_data[\"income_cat\"].where(housing_data[\"income_cat\"]<5, 5.0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets plot these and see what we have:\n",
    "\n",
    "housing_data[\"income_cat\"].hist(bins = 20)\n",
    "plt.title(\"Median Income Categories\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x = housing_data[\"ocean_proximity\"], data = housing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "So I have to split the data now into the training and test sets. However, special attention needs to be considered when doing this.\n",
    "\n",
    "Firstly, this is done to avoid Data Snooping Bias, which might occur if the overall data set is visualised now.\n",
    "\n",
    "Secondly, the test set must be split from the overall data, in such as a way, that the test set has the same distribution as the overall data in terms of the colummns and more especially for categorical features(columns).\n",
    "\n",
    "There are two methods we can use to split the data:\n",
    "1. Train Test Split class from Sklearn\n",
    "2. StratifiedShuffleSplit class from Sklearn.\n",
    "\n",
    "I have used the train test split more frequently, I will try the Stratified Shuffle Sampling split now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets use a Stratified Split based on the Income Categories\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split_2 = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
    "for train_index, test_index in split_2.split(housing_data, housing_data[\"income_cat\"]):\n",
    "    strat_train_set = housing_data.loc[train_index]\n",
    "    strat_test_set = housing_data.loc[test_index]\n",
    "print(len(strat_train_set), len(strat_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_data, test_data = train_test_split(housing_data, test_size = 0.2, random_state =42)\n",
    "print(len(training_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I will drop the Income Categories column from my training test(Strat Split)\n",
    "\n",
    "strat_train_set = strat_train_set.drop(\"income_cat\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set = strat_test_set.drop(\"income_cat\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets plot the training set to see what patterns are visible in the data, also we want to see the correlations between the attributes and the target attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind = \"scatter\", x = 'longitude', y = 'latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot gives a very basic render of the California region, and does not give any valuable information. So the parameters need to be added to improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind = \"scatter\", x = 'longitude', y = 'latitude', alpha = 0.1) # Check the density of the different districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "    sharex=False)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the we have a nice plot, and shows the population density, and the price of the houses in different districts. We have even takle it further and show an image of the California State and plot the plot the data on top of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "california_img=mpimg.imread('california.png')\n",
    "ax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n",
    "                       s=housing['population']/100, label=\"Population\",\n",
    "                       c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n",
    "                       colorbar=False, alpha=0.4,\n",
    "                      )\n",
    "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n",
    "           cmap=plt.get_cmap(\"jet\"))\n",
    "plt.ylabel(\"Latitude\", fontsize=14)\n",
    "plt.xlabel(\"Longitude\", fontsize=14)\n",
    "\n",
    "prices = housing[\"median_house_value\"]\n",
    "tick_values = np.linspace(prices.min(), prices.max(), 11)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\n",
    "cbar.set_label('Median House Value', fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Correlation\n",
    "\n",
    "Now I wanna see the correalations between the target attribute and the other attributes. There are two ways to do this:\n",
    "\n",
    "1. Get the Correlation Coefficient to see for linear correlations\n",
    "2. Plot a scatter matrix(pandas function), or pairplot(seaborn function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr() # Get the correlation matrix\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending = False) # See how the different attributes linearly correlate with the target attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median income has a positive correlation with the median house value. The r value is close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter matrix using Pandas\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"] # All these attributes have a positive correlation. \n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(housing[attributes], figsize = (15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to further investigate the relationship between the median income and median House value\n",
    "housing.plot(kind = \"scatter\", x = \"median_income\", y = \"median_house_value\", color = 'red' ,figsize = (10,7), alpha = 0.1)\n",
    "plt.title(\"Median Income vs Median House Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Attribute Combinations\n",
    "\n",
    "Now I will combine some attributes to see if the give some insights to this data. \n",
    "\n",
    "For example, the total number of rooms in a district is not very\n",
    "useful if you don’t know how many households there are. What you really want is the number of rooms\n",
    "per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to\n",
    "compare it to the number of rooms. And the population per household also seems like an interesting\n",
    "attribute combination to look at.\n",
    "\n",
    "So I will combine atrributes to get the:\n",
    "1. Number of rooms per household\n",
    "2. Number of bedrooms per number of rooms\n",
    "3. Population per Household\n",
    "\n",
    "Thereafter I want to see how these new attributes, correlate to the target attribute(house_value)\n",
    "\n",
    "#### NOTE: Exploratory Data Analysis is a iterative process. I might have to come back and explore gain with different combinations, depending on the type of project I am doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning and Preparation\n",
    "\n",
    "Now I will need to prepare the data for putting it in a ML Algo. \n",
    "There are a few things to note though:\n",
    "\n",
    "1. Firstly, I need to deal with missing data. I need to impute the missing values with the median value.\n",
    "2. Secondly,I need to deal with text or categorical data. For this, I will need to encode the data from categories to numbers, since ML algos work with numbers.\n",
    "\n",
    "So lets do that!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets find out which columns have missing values\n",
    "\n",
    "strat_train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing values with the median values for all attributes.\n",
    "\n",
    "housing_data = strat_train_set.drop(\"median_house_value\", axis = 1) # Need to drop the labels and work with just the features\n",
    "housing_data_labels = strat_train_set[\"median_house_value\"] # Get the labels\n",
    "housing_data_num = housing_data.drop(\"ocean_proximity\", axis = 1)# The imputer works with numerical values, so we will have drop the OceanProximity Column\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "X = imputer.fit_transform(housing_data_num) # Fit the data and then transform it to impute the missing values using median "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_tr = pd.DataFrame(X, columns = housing_data_num.columns)\n",
    "housing_data_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_tr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations\n",
    "\n",
    "Create a pipeline to automate transformations to data\n",
    "Transformations:\n",
    "\n",
    "1. Impute Missing Data Points with median values\n",
    "2. Do Feature Scaling: Standardisation\n",
    "3. Encode categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "feature_matrix = strat_train_set.drop(\"median_house_value\", axis = 1) # Drop the labels column\n",
    "label_vector = strat_train_set[\"median_house_value\"] # Get our labels vector\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_features = feature_matrix.drop(\"ocean_proximity\", axis = 1) # Get the numerical features\n",
    "#cat_features = feature_matrix[\"ocean_proximity\"] # Get the categorical features \n",
    "\n",
    "# Get a pipeline to perform transformations on the numerical features\n",
    "num_pipeline =Pipeline([\n",
    "    (\"imputer\",  SimpleImputer(strategy = \"median\")), # impute missing values with median\n",
    "    (\"scaler\", StandardScaler()) # Scale the data using Standardisation\n",
    "])\n",
    "\n",
    "# Use ColumnTransformer to encode the categorical data\n",
    "num_attr = list(num_features) # Put the column names in num_features in a list\n",
    "cat_attr = [\"ocean_proximity\"] # Get the name of the column with categorical features in a list\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attr),\n",
    "    (\"cat\", OneHotEncoder(), cat_attr)\n",
    "])\n",
    "\n",
    "feature_matrix = full_pipeline.fit_transform(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attr =list(num_features)\n",
    "print(num_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attr = [\"ocean_proximity\"]\n",
    "print(cat_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ShortList Some Promising Models\n",
    "\n",
    "Pick some models, and train them on the data and evaluate them. Usually pick between 2 and 5 models\n",
    "\n",
    "Models to work with:\n",
    "\n",
    "1. Linear Regression model\n",
    "2. Decision Tree model\n",
    "3. RandomForest Model\n",
    "4. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def train_eval(training_features, training_labels):\n",
    "    \n",
    "    models = [LinearRegression(), DecisionTreeRegressor(), RandomForestRegressor(), XGBRegressor()]\n",
    "    \n",
    "    for i in models:\n",
    "        #train models and conduct Cross Validation\n",
    "        i.fit(training_features, training_labels)\n",
    "        scores = cross_val_score(i, training_features, training_labels, cv = 5) # Conduct Cross validation on all the models\n",
    "        print(\"Scores:\", scores*100)\n",
    "        print(\"Standard deviation:\", (scores.std())*100)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Train and evaluate the models now\n",
    "\n",
    "train_eval(feature_matrix, label_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross_val_score using the neg_mean_squared_error, didnt work for some reason. The output of my training and evaulation function was NaNs. \n",
    "\n",
    "As a counter measure, I removed the scoring parameter. The result was a percentage score, for each fold.I then got the standard deviation. \n",
    "\n",
    "###### Model Performance:\n",
    "\n",
    "The Random Forest Regressor performed the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what I am talking about\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg_model = lin_reg.fit(feature_matrix, label_vector)\n",
    "lin_reg_scores = cross_val_score(lin_reg_model, feature_matrix, label_vector, scoring = \"neg_mean_squared_error\", cv = 5)\n",
    "lin_rmse = np.sqrt(lin_reg_scores)\n",
    "print(\"Scores:\", lin_rmse)\n",
    "print(\"Average:\", lin_rmse.mean())\n",
    "print(\"Standard deviation:\", lin_rmse.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine Tune the model\n",
    "\n",
    "Now that the model is trained. The best model can be fine tuned by choosing the best hyperparamters, that lead us to the best score\n",
    "\n",
    "There are two ways to do this:\n",
    "\n",
    "1. GridSearch Cross Validation\n",
    "\n",
    "2. RandomisedSearch.\n",
    "\n",
    "One basically searches for the best hyperparamters, that will gives the best scores.\n",
    "\n",
    "As previously stated, the RandomForest Regressor performed the best, so it will be fine tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Lets the hyperparameter values, and put them in a list of dictionaries\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "randomforest_regressor = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(randomforest_regressor, param_grid, cv = 5, scoring = \"neg_mean_squared_error\", return_train_score = True)\n",
    "\n",
    "grid_search.fit(feature_matrix, label_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid =  {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}\n",
    "   \n",
    "    #{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}\n",
    "\n",
    "\n",
    "\n",
    "rand_search = RandomizedSearchCV(randomforest_regressor, param_grid, cv = 5, scoring = \"neg_mean_squared_error\", return_train_score = True)\n",
    "rand_search.fit(feature_matrix, label_vector)\n",
    "rand_search.get_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets make some predictions on the test set\n",
    "\n",
    "test_features = strat_test_set.drop(\"median_house_value\", axis = 1)\n",
    "test_labels = strat_test_set[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the test set using our transformation pipeline\n",
    "test_features = full_pipeline.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest_regressor = RandomForestRegressor(n_estimators = 30, max_features = 8)\n",
    "\n",
    "model = randomforest_regressor.fit(feature_matrix, label_vector)\n",
    "predictions = model.predict(test_features)\n",
    "\n",
    "print(\"True Target:\", test_labels, \"Predicted Target:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "accuracy = mean_squared_error(test_labels, predictions)\n",
    "rmse = np.sqrt(accuracy)\n",
    "print(\"Accuracy Score:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, one can further improve the model, pick a different model etc, until a satisfactory result is achieved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Try a Support Vector Machine regressor (sklearn.svm.SVR) with\n",
    "various hyperparameters, such as kernel=\"linear\" (with various\n",
    "values for the C hyperparameter) or kernel=\"rbf\" (with various\n",
    "values for the C and gamma hyperparameters). Don’t worry about what\n",
    "these hyperparameters mean for now. How does the best SVR predictor\n",
    "perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Linear Kernel and various c values(Will use GridSearchCV)\n",
    "\n",
    "# Step 1: Find the best kernel and C values. \n",
    "\n",
    "# Get the hyperparameters we want to check: The kernel, the C value and the gamma value(Find out what C and gamma do)\n",
    "param_grid = [\n",
    "        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n",
    "        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n",
    "         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n",
    "    ]\n",
    "\n",
    "# Set up the svr object:\n",
    "\n",
    "svm_regressor = SVR()\n",
    "\n",
    "grid_search_svr = GridSearchCV(svm_regressor, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "\n",
    "grid_search_svr.fit(feature_matrix, label_vector)\n",
    "grid_search_svr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_mse = grid_search_svr.best_score_\n",
    "rmse = np.sqrt(-negative_mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets test how the svr does on the test set:\n",
    "\n",
    "svm_regressor = SVR()\n",
    "\n",
    "model_svr = svm_regressor.fit(feature_matrix, label_vector)\n",
    "predictions_svm = model.predict(test_features)\n",
    "score_svm = mean_squared_error(test_labels, predictions_svm)\n",
    "print(\"SVM Regressor Error:\", score_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Try replacing GridSearchCV with RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the hyperparamter values, because it is REALLY is computationally expensive\n",
    "from scipy.stats import expon, reciprocal\n",
    "param_distribs = {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': reciprocal(20, 200000),\n",
    "        'gamma': expon(scale=1.0),\n",
    "    }\n",
    "\n",
    "randsvm_search = RandomizedSearchCV(svm_regressor, param_distributions=param_distribs,\n",
    "                                n_iter=50, cv=5, scoring='neg_mean_squared_error',\n",
    "                                verbose=2, n_jobs=-1, random_state=42)\n",
    "randsvm_search.fit(feature_matrix, label_vector)\n",
    "randsvm_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_mse_2 = randsvm_search.best_score_\n",
    "rmse_2 = np.sqrt(-negative_mse_2)\n",
    "rmse_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the Random Search, we use distributions of the hyperparamter values. We can use different distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import geom, expon\n",
    "geom_distrib=geom(0.5).rvs(10000, random_state=42)\n",
    "expon_distrib=expon(scale=1).rvs(10000, random_state=42)\n",
    "plt.hist(geom_distrib, bins=50)\n",
    "plt.show()\n",
    "plt.hist(expon_distrib, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Gamma Parameter, we used the exponential distribution, with a scale of 1.0, using 10000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expon_distrib = expon(scale=1.)\n",
    "samples = expon_distrib.rvs(10000, random_state=42)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Exponential distribution (scale=1.0)\")\n",
    "plt.hist(samples, bins=50)\n",
    "plt.subplot(122)\n",
    "plt.title(\"Log of this distribution\")\n",
    "plt.hist(np.log(samples), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the C hyperparameter, we use the reciprocal distribution, for ranges of C between 20 and 20000, using 10000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reciprocal_distrib = reciprocal(20, 200000)\n",
    "samples = reciprocal_distrib.rvs(10000, random_state=42)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Reciprocal distribution (scale=1.0)\")\n",
    "plt.hist(samples, bins=50)\n",
    "plt.subplot(122)\n",
    "plt.title(\"Log of this distribution\")\n",
    "plt.hist(np.log(samples), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reciprocal distribution is useful when you have no idea what the scale of the hyperparameter should be (indeed, as you can see on the figure on the right, all scales are equally likely, within the given range), whereas the exponential distribution is best when you know (more or less) what the scale of the hyperparameter should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEED TO REVIEW THE DOCS FOR THESE DISTRIBUTIONS BRO!!!\n",
    "\n",
    "see https://docs.scipy.org/doc/scipy/reference/stats.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Try adding a transformer in the preparation pipeline to select only the\n",
    "most important attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add the ability to determine the most important features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person():\n",
    "    def __init__(self, name, surname):\n",
    "        self.name = name\n",
    "        self.surname = surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hunter = Person(\"Mbasa\", \"Cokile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hunter.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hunter.name, hunter.surname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Car(): # Parent Class\n",
    "    def exclaim(self):\n",
    "        print(\"I am a Car!\")\n",
    "\n",
    "class Yugo(Car):# Child Class\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "give_me_yugo = Yugo()\n",
    "give_me_yugo.exclaim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "give_me_car = Car().exclaim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def exclaim(self):\n",
    "        print(\"HAHAHA, really!!\")\n",
    "class MDPerson(Person):\n",
    "    def __init__(self, name, position):\n",
    "        super().__init__(name)\n",
    "        self.position  = position \n",
    "class JDPerson(Person):\n",
    "    def __init__(self, name, position):\n",
    "        super().__init__(name)\n",
    "        self.position = position\n",
    "\n",
    "# We have changed the initialisation method(), in the child classes. Lets test it mate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Objects of each class\n",
    "someone = Person(\"Mike Oxsmall\")\n",
    "\n",
    "doctor = MDPerson(\"Mike Oxsmall\", \"Doctor\")\n",
    "\n",
    "lawyer = JDPerson(\"Mike Oxsmall\", \"Laywer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the name attributes of each object\n",
    "\n",
    "print(someone.name)\n",
    "print(doctor.position, doctor.name)\n",
    "print(lawyer.name, lawyer.position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person_1():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "class EmailPerson(Person_1):\n",
    "    def __init__(self, name, email):\n",
    "        super().__init__(name)\n",
    "        self.email = email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = EmailPerson(\"Mbasa Cokile\", \"mbasacokile7@yahoo.com\")\n",
    "print(details.name, details.email)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_1 = Person(\"Mike Oxsmall\")\n",
    "Person.exclaim(person_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = Person(\"Mike Oxsmall\")\n",
    "Person.exclaim(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = Person(\"Cory Chatsworth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.name = \"Mike Oxmall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(person.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Types of methods:\n",
    "1. Object Method (Always has the `self` argument in the function def)\n",
    "2. Class Method, has a preceding decorator(`@classmethod`), and the initial argument is the class itself, using the keyword: `cls`\n",
    "3. Static Method, no arguments and also has a preceding decorator (`@staticmethod`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A():\n",
    "    count = 0\n",
    "    def __init__(self): # Object Method\n",
    "        A.count +=1 # Class Attribute\n",
    "    def exclaim(self): # Object Method\n",
    "        print(\"I'm an A bruv, BOOM!\")\n",
    "    @classmethod\n",
    "    def kids(cls): # Class Method\n",
    "        print(\"A has \", cls.count, \"Little Objects\") #Prints out how many objects the class has\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some objects\n",
    "breezy_A = A()\n",
    "Eazy_A = A()\n",
    "Skilly_A = A()\n",
    "Filly_A = A()\n",
    "Freakin_A = A()\n",
    "\n",
    "# use the class method now:\n",
    "\n",
    "A.kids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Python Exercises(Chapter 6: OOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "Make a class called Thing with no contents and print it. Then, create an object called\n",
    "example from this class and also print it. Are the printed values the same or different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Thing object at 0x0000020C8B72FE88>\n"
     ]
    }
   ],
   "source": [
    "class Thing():\n",
    "    pass\n",
    "\n",
    "print(Thing())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Thing object at 0x0000020C8B73E748>\n"
     ]
    }
   ],
   "source": [
    "example = Thing()\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the printed the values are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "    \n",
    "Make a new class called Thing2 and assign the value 'abc' to a class attribute called\n",
    "letters. Print letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n"
     ]
    }
   ],
   "source": [
    "class Thing2():\n",
    "    letters = \"abc\"\n",
    "\n",
    "print(Thing2.letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "Make yet another class called, of course, Thing3. This time, assign the value 'xyz'\n",
    "to an instance (object) attribute called letters. Print letters. Do you need to make\n",
    "an object from the class to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xyz\n"
     ]
    }
   ],
   "source": [
    "class Thing3():\n",
    "    def __init__(self):\n",
    "        self.letters = \"xyz\" # Object Attribute\n",
    "\n",
    "\n",
    "\n",
    "something = Thing3()\n",
    "print(something.letters)\n",
    "# Awe, so the attribute is for an object created from the class, so you have to make an object to print the letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "\n",
    "Make a class called Element, with instance attributes name, symbol, and number.\n",
    "Create an object of this class with the values 'Hydrogen', 'H', and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Element():\n",
    "    def __init__(self, name, symbol, number):\n",
    "        self.name = name\n",
    "        self.symbol = symbol\n",
    "        self.number = number\n",
    "        \n",
    "element = Element(\"Hydrogen\", \"H\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "Make a dictionary with these keys and values: 'name': 'Hydrogen', 'symbol':\n",
    "'H', 'number': 1. Then, create an object called hydrogen from class Element using\n",
    "this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_dict = {'name': \"Hydrogen\", 'symbol': \"H\", \"number\": 1}\n",
    "\n",
    "hydrogen = Element(**element_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hydrogen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrogen.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "For the Element class, define a method called dump() that prints the values of the\n",
    "object’s attributes (name, symbol, and number). Create the hydrogen object from this new\n",
    "definition and use dump() to print its attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  Hydrogen , Symbol: H , Number:  1\n"
     ]
    }
   ],
   "source": [
    "class Element():\n",
    "    def __init__(self, name, symbol, number):\n",
    "        self.name = name\n",
    "        self.symbol = symbol\n",
    "        self.number = number\n",
    "        \n",
    "    def dump(self):\n",
    "        print(\"Name: \",self.name,\", Symbol:\",self.symbol,\", Number: \",self.number)\n",
    "        \n",
    "hydrogen = Element(\"Hydrogen\", \"H\", 1)\n",
    "hydrogen.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The author did it like this:\n",
    "\n",
    "def dump(self):\n",
    "    print('name=%s, symbol=%s, number=%s' % (self.name, self.symbol, self.number))\n",
    "    \n",
    "# Basically, does the same thing, as long we got to print the attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "\n",
    "Call print(hydrogen). In the definition of Element, change the name of method\n",
    "dump to __str__, create a new hydrogen object, and call print(hydrogen) again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Element object at 0x0000020C8B7E7088>\n"
     ]
    }
   ],
   "source": [
    "print(hydrogen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Element():\n",
    "    def __init__(self, name, symbol, number):\n",
    "        self.name = name\n",
    "        self.symbol = symbol\n",
    "        self.number = number\n",
    "        \n",
    "    def __str__(self):\n",
    "        return('name=%s, symbol=%s, number=%s' % \n",
    "              (self.name, self.symbol, self.number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name=Hydrogen, symbol=H, number=1\n"
     ]
    }
   ],
   "source": [
    "hydrogen = Element(\"Hydrogen\", \"H\", 1)\n",
    "print(hydrogen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8\n",
    "\n",
    "Modify Element to make the attributes name, symbol, and number private. Define a\n",
    "getter property for each to return its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Element():\n",
    "    def __init__(self, name, symbol, number):\n",
    "        self.__name = name\n",
    "        self.__symbol = symbol\n",
    "        self.__number = number\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__name\n",
    "    \n",
    "    @property\n",
    "    def symbol(self):\n",
    "        return self.__symbol\n",
    "    \n",
    "    @property\n",
    "    def number(self):\n",
    "        return self.__number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrogen = Element(**element_dict)\n",
    "\n",
    "hydrogen.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hydrogen'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrogen.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrogen.symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9\n",
    "\n",
    "Define three classes: Bear, Rabbit, and Octothorpe. For each, define only one\n",
    "method: eats(). This should return 'berries' (Bear), 'clover' (Rabbit), or\n",
    "'campers' (Octothorpe). Create one object from each and print what it eats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bear():\n",
    "    def eats(self):\n",
    "        return \"Bears eat berries\"\n",
    "\n",
    "class Rabbit():\n",
    "    def eats(self):\n",
    "        return \"Rabbits eat clovers\"\n",
    "\n",
    "class Octothorpe():\n",
    "    def eats(self):\n",
    "        return \"Octothorpes eat campers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bear eat berries\n"
     ]
    }
   ],
   "source": [
    "bear = Bear()\n",
    "print(bear.eats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rabbits eat clovers\n"
     ]
    }
   ],
   "source": [
    "rabbit = Rabbit()\n",
    "print(rabbit.eats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Octothorpes eat campers\n"
     ]
    }
   ],
   "source": [
    "octopus = Octothorpe()\n",
    "print(octopus.eats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10\n",
    "\n",
    "Define these classes: Laser, Claw, and SmartPhone. Each has only one method:\n",
    "does(). This returns 'disintegrate' (Laser), 'crush' (Claw), or 'ring' (Smart\n",
    "Phone). Then, define the class Robot that has one instance (object) of each of these.\n",
    "Define a does() method for the Robot that prints what its component objects do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Laser():\n",
    "    def does(self):\n",
    "        return \"Disintegrate\"\n",
    "    \n",
    "class Claw ():\n",
    "    def does(self):\n",
    "        return \"Crush\"\n",
    "    \n",
    "class Smartphone():\n",
    "    def does(self):\n",
    "        return \"Ring\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Robot:\n",
    "    def __init__(self):\n",
    "        self.laser = Laser()\n",
    "        self.claw = Claw()\n",
    "        self.smartphone = Smartphone()\n",
    "        \n",
    "    def does(self):\n",
    "        return '''I have many attachmen My laser, to %s. My claw, to %s. My smartphone, to %s.''' % (self.laser.does(), self.claw.does(),self.smartphone.does() )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have many attachmen My laser, to Disintegrate. My claw, to Crush. My smartphone, to Ring.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robot = Robot()\n",
    "\n",
    "robot.does()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
